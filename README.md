
 <div align='center' > 
  <h2> IEEE International Conference on Multimedia and Expo </h2>
 </div>

 <div align='center' style = "vertical-align:middle"> 
  <h2> <img src="./picture/icme.png" margn-right="20px" ><a href="https://2022.ieeeicme.org/"> ICME 2022 </a><a href="https://2022.ieeeicme.org/">(https://2022.ieeeicme.org/)</a> </h2>
 </div>
 
 <div align='center' > 
  <h2> Special Session on </h2>
  <h2> Beyond Accuracy: Responsible, Responsive, and Robust Multimedia Retrieval
</h2>
 </div>


The accept papers will be the oral paper at ICME (top 15%), and go through the same peer review process as the regular papers. If a special session has more than 5 papers being accepted, some of the papers will be moved to the regular paper sessions of the conference.

## Abstract:
In this special session, we aim to bring together the latest advances in Responsible, Responsive, and Robust (3R) multimedia retrieval, and draw attention from both the academic and industrial communities. A Responsible system aims to protect users’ privacy and related rights; a Responsive system focuses on the efficient and effective feedback on million-scale input data; while a Robust system ensures reliability and reproducibility of predictions and avoids unnecessary fatal errors.

1.	Responsible: The privacy of user data must be respected and used in beneficial contexts, especially the biometric data of users such as the facial images. There are two potential solutions to protect user privacy. One is to leverage the generated data by GAN or the synthetic data by the 3D engine for the model training, where the model does not need knowledge of user data. Another solution is to harness the Federated Learning with fewer demands to user’s privacy. These two approaches are still under-explored. Through the special session, we hope to provide an avenue for the community to discuss the development and draw attention to a responsible multimedia retrieval system.

2.	Responsive: The increasing quantity of multimedia data also demands an efficient retrieval system, which can handle the million-scale input data in response time during user interactions. It remains unknown whether the learned representation by CNN, RNN and transformers is compatible with the traditional hashing approaches or other dimension reduction methods, like PCA. On the other hand, there is also the scientific question of efficient model design. The related techniques include automated machine learning (auto-ML), the model pruning for CNN, RNN and transformers, and the prompt-based learning to save the training or testing time.

3.	Robustness: It remains a great challenge to train system to deal with the out-of-distribution data. Two cases in recent years show that the systems trained via blind data-driven approaches may lead to unexpected and undesirable results. In 2015, one commercial photo system labels the African Americans as gorillas, which raises great concerns about the racial discrimination and biases of such human recognition systems. Since the system is blindly trained by the data and hard to tune, one quick fix to alleviate model biases at that time was to remove the class of gorillas in the system. Similarly, in 2018, a self-driving car hits a pedestrian due to the mis-classification of the pedestrian with her bike, which may not be seen by the system during training. If both systems could consider uncertainty and learn more invariant causal factors, such accidents can be avoided. Therefore, we also want to promote discussions on viewpoint-invariant representation learning, domain adaptation, and long-tailed recognition for multimedia retrieval. 


**The list of possible topics includes, but is not limited to:**

-	Responsible: Federal Learning for Multimedia Applications
-	Responsible: Synthetic Data / Generated Data for Representation Learning
-	Responsible: Interpretable Multimedia Learning and Visualization Techniques
-	Responsible: Human-Centered Multimedia Analysis
-	Responsive: Model Compression / AutoML for Multimedia Retrieval
-	Responsive: New Hashing Methods for Representation Learning
-	Responsive: Prompt Learning for Multi-Domain Adaptation 
-	Responsive: Multimedia Computing on Edge Devices
-	Robust: New Evaluation Metrics and Benchmarks
-	Robust: New Metric Learning Methods 
-	Robust: Causal Learning
-	Robust: Uncertainty-based Model Evaluation / Training
-	Robust: New Adversarial Samples / Defense Methods for Multimedia Retrieval

**Reference:**

[1] Kendall A, Gal Y. What uncertainties do we need in bayesian deep learning for computer vision? NeurIPS, 2017.

[2] Zheng Z, Yang Y. Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation. IJCV, 2021. 

[3] Four Principles of Explainable Artificial Intelligence (https://nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf)

[4] Zheng, Z., Ruan, T., Wei, Y., Yang, Y., & Mei, T. VehicleNet: Learning robust visual representation for vehicle re-identification. TMM, 2020.

**Tips:**
- For privacy protection, please blur faces in the published materials (such as paper, video, poster, etc.) 
- For social good, please do not contain any misleading words, such as ``surveillance`` and  ``secret``.

## Important Dates

**Submission of papers:**

* New Regular Paper Submission deadline: December 22, 2021 [11:59 p.m. PST]
* Paper acceptance notification: March 5, 2022

## Organizing Team

| <img src="./picture/1.png" width="160"> |<img src="./picture/2.png" width="160"> |<img src="http://zheng-lab.cecs.anu.edu.au/1.jpg" width="160"> |
| :-: | :-: | :-: |
|  [Zhedong Zheng](zdzheng.xyz), National University of Singapore, Singapore | [Linchao Zhu](http://ffmpbgrnn.github.io), University of Technology Sydney, Australia | [Liang Zheng](https://zheng-lab.cecs.anu.edu.au), Australian National University, Australia |
| <img src="./picture/4.png" width="160"> |  <img src="./picture/5.png" width="160"> |
|  [Yi Yang](https://scholar.google.com/citations?user=RMSuNFwAAAAJ), Zhejiang University, China | [Tat-Seng Chua](https://www.chuatatseng.com), National University of Singapore, Singapore |


## Conference and Journal Papers

***The special session submission site is open and can be accessed at <a href="https://cmt3.research.microsoft.com/ICME2022">https://cmt3.research.microsoft.com/ICME2022</a>. Under "Create New Submission", the authors need to choose the "subject area" as the desired "special session x", so the papers can be counted as special sessions.  If they choose other subject areas, the papers will belong to the main program.***

All papers presented at ICME 2022 will be included in IEEE Xplore. All papers submitted to this special session will go through the same review process as the regular papers submitted to the main conference to ensure that the contributions are of high quality. If a special session has more than 5 papers being accepted, some of the papers will be moved to the regular paper sessions of the conference.
The IEEE Transactions on Multimedia is partnering with IEEE ICME. Extended versions of the top-ranked ICME 2022 papers will be invited for submission and potential publication in the IEEE Transactions on Multimedia and IEEE Open Journal of Circuits and Systems. We invite authors to submit high-quality contributions aiming at taking part in this call and to have the opportunity to publish an extended ICME paper in this prestigious journal. After the due review process, if your paper is highly ranked, the Technical Program Committee Chairs will get in touch with you regarding the next steps.
